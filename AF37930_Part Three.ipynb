{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b8db90-0e00-4b3a-845f-b042c68d865b",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "# BERT Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33605e5a-4428-444e-b386-131bcc8a7c5b",
   "metadata": {},
   "source": [
    "## 5.1 Base BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d9818-588b-41db-b1ba-dfe06e9b9088",
   "metadata": {},
   "source": [
    "### 5.1.1 Import Packages & Setup\n",
    "First, I installed the  TensorFlow library to enable building and training deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68fd14b0-5594-4e3f-814b-b735bb9c85f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizerFast, BertPreTrainedModel, BertModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import AutoConfig\n",
    "\n",
    "# Read in three datasets\n",
    "data_train = pd.read_csv('data_train.csv')\n",
    "data_val = pd.read_csv('data_val.csv')\n",
    "data_test = pd.read_csv('data_test.csv')\n",
    "\n",
    "# For build\n",
    "# data_train = data_train.sample(frac=0.003, random_state=42).reset_index(drop=True)\n",
    "# data_val = data_val.sample(frac=0.05, random_state=42).reset_index(drop=True)\n",
    "# data_test = data_test.sample(frac=0.01, random_state=42).reset_index(drop=True)\n",
    "\n",
    "full_cols = [\n",
    "    'sdoh_community_present', 'sdoh_community_absent', 'sdoh_education',\n",
    "    'sdoh_economics', 'sdoh_environment', 'behavior_alcohol',\n",
    "    'behavior_tobacco', 'behavior_drug'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f39bbe7-e27c-4fad-b1e6-8b7dfd8d95c5",
   "metadata": {},
   "source": [
    "#### Convert to Hugging Face Dataset\n",
    "Using the Dataset makes handling data is easier to use Hugging Face training tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f2d3234-0365-4282-b26d-14dce1f7ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrames to Hugging Face Dataset\n",
    "train_ds = Dataset.from_pandas(data_train)\n",
    "val_ds = Dataset.from_pandas(data_val)\n",
    "test_ds = Dataset.from_pandas(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ddf205-ff99-44e5-8a81-4b1b51015dd7",
   "metadata": {},
   "source": [
    "#### Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4ca27c-6e8f-4d80-a2d2-38cbda4ee28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4244d9ef66e2469c913594c9a8ba04fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4917 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807b93d363ea4ae7be0c7caf8a7b0596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1055 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006d643694bc4f15a072dedba19ab298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1053 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b43b507a7d443fbf1cc5d32bfa1d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4917 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27145536929142c1b86c5f66f4f56cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1055 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f2ba1fa8cf4e5a9546433967aa69a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1053 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "# 2. tokenizer\n",
    "modelname = 'bert-base-uncased'\n",
    "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['social_history'], padding='max_length',truncation=True,max_length=128)\n",
    "    \n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "val_ds = val_ds.map(tokenize, batched=True)\n",
    "test_ds = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "def combine_labels(batch):\n",
    "    labels = []\n",
    "    for i in range(len(batch[full_cols[0]])):\n",
    "        row = []\n",
    "        for cols in full_cols:\n",
    "            row.append(batch[cols][i])\n",
    "        labels.append(row)\n",
    "    batch['labels'] = labels\n",
    "    return batch\n",
    "\n",
    "train_ds = train_ds.map(combine_labels, batched=True)\n",
    "val_ds = val_ds.map(combine_labels, batched=True)\n",
    "test_ds = test_ds.map(combine_labels, batched=True)\n",
    "\n",
    "\n",
    "# Remove extra columns to avoid passing extra columns to model\n",
    "use_cols=['input_ids', 'attention_mask', 'labels']\n",
    "train_ds = train_ds.remove_columns([cols for cols in train_ds.column_names if cols not in use_cols])\n",
    "val_ds = val_ds.remove_columns([cols for cols in val_ds.column_names if cols not in use_cols])\n",
    "test_ds = test_ds.remove_columns([cols for cols in test_ds.column_names if cols not in use_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35d7c7c-ec37-4108-8037-894dd5119b73",
   "metadata": {},
   "source": [
    "Define a simple tokenisation function that takes a batch of text and labels as input, takes out the text part of it, and returns the tokenised text and the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7f88a9-6b76-4813-8fa5-a4ac89a6fac9",
   "metadata": {},
   "source": [
    "### 5.1.2 Create the Standard BERT Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f044b4a-cced-4a5e-855e-ca1e04e04439",
   "metadata": {},
   "source": [
    "Similarly, I build a model that shares a single encoder and uses a separate classification head (nn.Linear) for each task\n",
    "- Merges logits from all tasks into a unified tensor with padding for alignment\n",
    "- Supports joint loss computation across tasks using CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aa1b118-dd72-4bf1-8556-c0ec6f10fc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MultiTaskBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifiers.0.bias', 'classifiers.0.weight', 'classifiers.1.bias', 'classifiers.1.weight', 'classifiers.2.bias', 'classifiers.2.weight', 'classifiers.3.bias', 'classifiers.3.weight', 'classifiers.4.bias', 'classifiers.4.weight', 'classifiers.5.bias', 'classifiers.5.weight', 'classifiers.6.bias', 'classifiers.6.weight', 'classifiers.7.bias', 'classifiers.7.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels_list=[2,2,2,3,3,5,5,5]\n",
    "\n",
    "# Use the pretrained BertPreTrainedModel class for initialization**\n",
    "class MultiTaskBertModel(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels_list = num_labels_list\n",
    "        self.num_tasks = 8\n",
    "        \n",
    "        #self.bert = AutoModel.from_config(config)\n",
    "        self.bert = AutoModel.from_pretrained('bert-base-uncased', config=config)\n",
    "\n",
    "        # Add a Hidden layer before classification heads\n",
    "        #self.hidden = nn.Sequential(\n",
    "            #nn.Linear(config.hidden_size, 256),\n",
    "            #nn.GELU(),\n",
    "            # Add a 50% dropout\n",
    "            #nn.Dropout(0.5)\n",
    "        #)\n",
    "        \n",
    "        # Use one classification head per task and register them using nn.ModuleList.\n",
    "        self.classifiers = nn.ModuleList([nn.Linear(config.hidden_size, num_labels) for num_labels in num_labels_list])\n",
    "        #self.classifiers = nn.ModuleList([nn.Linear(256, num_labels) for num_labels in num_labels_list])\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "        \n",
    "    # Define forward pass for multi-task model to compute combined loss for joint training\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        # hidden_output = self.hidden(pooled_output)\n",
    "        logits_list = [classifier(pooled_output) for classifier in self.classifiers]\n",
    "\n",
    "        batch_size = pooled_output.size(0)\n",
    "        device = pooled_output.device\n",
    "\n",
    "        # As Using Trainer needs unified logits with consistent shape across tasks. Combine logits from 8 tasks and use -1e9 as padding to mask unused positions.\n",
    "        # maximun of num_labels is 5\n",
    "\n",
    "        # Initialize logits with zeros, shape: (batch_size, num_tasks, max_labels)\n",
    "        logits = torch.zeros(batch_size, self.num_tasks, 5, device=device)\n",
    "        #logits = torch.full((batch_size, self.num_tasks, 5), -1e9, device=device)\n",
    "        for i, logit_task in enumerate(logits_list):\n",
    "\n",
    "            num_labels = self.num_labels_list[i]\n",
    "            #print(f\"Task {i} logits shape: {logit_task.shape}\")\n",
    "            logits[:, i, :num_labels] = logit_task\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = 0\n",
    "        for i in range(self.num_tasks):\n",
    "            loss += loss_fct(logits[:, i, :self.num_labels_list[i]], labels[:, i])\n",
    "\n",
    "        return (loss, logits)\n",
    "\n",
    "config = AutoConfig.from_pretrained('bert-base-uncased')\n",
    "model = MultiTaskBertModel.from_pretrained('bert-base-uncased', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761116f8-9ccf-43da-b0cc-ae7623cbcb4d",
   "metadata": {},
   "source": [
    "### 5.1.3 Set Up Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad53a5f3-b72c-4b61-b7b4-cebe3157c4e5",
   "metadata": {},
   "source": [
    "Same as above, macro and weighted scores are chosen to evaluate model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd8521f8-6340-4b8d-8c7c-ff84be701c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    labels = labels.astype(int)\n",
    "\n",
    "    precision_macro, recall_macro, f1_macro = [], [], []\n",
    "    precision_weighted, recall_weighted, f1_weighted = [], [], []\n",
    "    acc_list = []\n",
    "\n",
    "    # 用于保存每个任务的 Macro F1 分数\n",
    "    task_f1_scores = {}\n",
    "\n",
    "    for i in range(8):\n",
    "        logit_task = logits[:, i, :num_labels_list[i]]\n",
    "        preds_i = np.argmax(logit_task, axis=1)\n",
    "        y_true = labels[:, i]\n",
    "\n",
    "        acc = accuracy_score(y_true, preds_i)\n",
    "        acc_list.append(acc)\n",
    "\n",
    "        # macro\n",
    "        prec = precision_score(y_true, preds_i, average='macro', zero_division=0)\n",
    "        rec = recall_score(y_true, preds_i, average='macro', zero_division=0)\n",
    "        f1 = f1_score(y_true, preds_i, average='macro', zero_division=0)\n",
    "        precision_macro.append(prec)\n",
    "        recall_macro.append(rec)\n",
    "        f1_macro.append(f1)\n",
    "        \n",
    "\n",
    "        # 保存每个任务的 Macro F1 score，使用 full_cols[i] 作为任务的名称\n",
    "        task_f1_scores[f'{full_cols[i]}_macro_f1'] = f1  # 使用任务名称作为键\n",
    "\n",
    "        # weighted\n",
    "        prec_w = precision_score(y_true, preds_i, average='weighted', zero_division=0)\n",
    "        rec_w = recall_score(y_true, preds_i, average='weighted', zero_division=0)\n",
    "        f1_w = f1_score(y_true, preds_i, average='weighted', zero_division=0)\n",
    "        precision_weighted.append(prec_w)\n",
    "        recall_weighted.append(rec_w)\n",
    "        f1_weighted.append(f1_w)\n",
    "\n",
    "    metrics = {\n",
    "        'macro_acc': np.mean(acc_list),\n",
    "\n",
    "        'macro_precision': np.mean(precision_macro),\n",
    "        'macro_recall': np.mean(recall_macro),\n",
    "        'macro_f1': np.mean(f1_macro),\n",
    "\n",
    "        'weighted_precision': np.mean(precision_weighted),\n",
    "        'weighted_recall': np.mean(recall_weighted),\n",
    "        'weighted_f1': np.mean(f1_weighted),\n",
    "    }\n",
    "\n",
    "    metrics.update(task_f1_scores)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9421ad-e2d2-4d15-a53a-3b9508179dc4",
   "metadata": {},
   "source": [
    "### 5.1.4 Set up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0a539cd-3d81-4826-8f43-699fec695589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a few arguments in a seperate TrainingArguments class\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./Final_Project',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='steps',    \n",
    "    logging_steps=50,\n",
    "    push_to_hub=False,\n",
    "    #report_to=\"tensorboard\",\n",
    "    #Evaluate after each epoch and save checkpoints to easily resume training, add epochs\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "\n",
    "    # Set to automatically load the the best macro_f1 score model on validation set\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='macro_f1',\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    # tokenizer=tokenizer,\n",
    "    processing_class= tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322255fa-139d-49a8-8110-ce1c10d968ce",
   "metadata": {},
   "source": [
    "### 5.1.5 Train the Module & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1660f643-2b6a-45ee-b581-39243939bc76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4620' max='4620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4620/4620 4:18:52, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro Acc</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Weighted Precision</th>\n",
       "      <th>Weighted Recall</th>\n",
       "      <th>Weighted F1</th>\n",
       "      <th>Sdoh Community Present Macro F1</th>\n",
       "      <th>Sdoh Community Absent Macro F1</th>\n",
       "      <th>Sdoh Education Macro F1</th>\n",
       "      <th>Sdoh Economics Macro F1</th>\n",
       "      <th>Sdoh Environment Macro F1</th>\n",
       "      <th>Behavior Alcohol Macro F1</th>\n",
       "      <th>Behavior Tobacco Macro F1</th>\n",
       "      <th>Behavior Drug Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.910400</td>\n",
       "      <td>3.701662</td>\n",
       "      <td>0.848934</td>\n",
       "      <td>0.655736</td>\n",
       "      <td>0.578165</td>\n",
       "      <td>0.566819</td>\n",
       "      <td>0.828689</td>\n",
       "      <td>0.848934</td>\n",
       "      <td>0.823307</td>\n",
       "      <td>0.910095</td>\n",
       "      <td>0.557343</td>\n",
       "      <td>0.492300</td>\n",
       "      <td>0.550012</td>\n",
       "      <td>0.607533</td>\n",
       "      <td>0.475393</td>\n",
       "      <td>0.562691</td>\n",
       "      <td>0.379181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.514600</td>\n",
       "      <td>2.322437</td>\n",
       "      <td>0.916825</td>\n",
       "      <td>0.806499</td>\n",
       "      <td>0.718903</td>\n",
       "      <td>0.740230</td>\n",
       "      <td>0.910222</td>\n",
       "      <td>0.916825</td>\n",
       "      <td>0.909670</td>\n",
       "      <td>0.973265</td>\n",
       "      <td>0.876270</td>\n",
       "      <td>0.647328</td>\n",
       "      <td>0.838419</td>\n",
       "      <td>0.632662</td>\n",
       "      <td>0.670736</td>\n",
       "      <td>0.778025</td>\n",
       "      <td>0.505133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.664400</td>\n",
       "      <td>1.758617</td>\n",
       "      <td>0.936611</td>\n",
       "      <td>0.852275</td>\n",
       "      <td>0.786118</td>\n",
       "      <td>0.804176</td>\n",
       "      <td>0.933339</td>\n",
       "      <td>0.936611</td>\n",
       "      <td>0.932459</td>\n",
       "      <td>0.979435</td>\n",
       "      <td>0.944930</td>\n",
       "      <td>0.828956</td>\n",
       "      <td>0.892471</td>\n",
       "      <td>0.639514</td>\n",
       "      <td>0.770375</td>\n",
       "      <td>0.826875</td>\n",
       "      <td>0.550853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.328400</td>\n",
       "      <td>1.532853</td>\n",
       "      <td>0.942417</td>\n",
       "      <td>0.887173</td>\n",
       "      <td>0.815248</td>\n",
       "      <td>0.832272</td>\n",
       "      <td>0.940539</td>\n",
       "      <td>0.942417</td>\n",
       "      <td>0.940114</td>\n",
       "      <td>0.977352</td>\n",
       "      <td>0.950455</td>\n",
       "      <td>0.864768</td>\n",
       "      <td>0.886598</td>\n",
       "      <td>0.704612</td>\n",
       "      <td>0.821946</td>\n",
       "      <td>0.852799</td>\n",
       "      <td>0.599649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.953300</td>\n",
       "      <td>1.396813</td>\n",
       "      <td>0.947986</td>\n",
       "      <td>0.897378</td>\n",
       "      <td>0.838233</td>\n",
       "      <td>0.854001</td>\n",
       "      <td>0.946808</td>\n",
       "      <td>0.947986</td>\n",
       "      <td>0.946342</td>\n",
       "      <td>0.981575</td>\n",
       "      <td>0.946323</td>\n",
       "      <td>0.903317</td>\n",
       "      <td>0.890066</td>\n",
       "      <td>0.757534</td>\n",
       "      <td>0.842351</td>\n",
       "      <td>0.877696</td>\n",
       "      <td>0.633145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.782600</td>\n",
       "      <td>1.305240</td>\n",
       "      <td>0.950474</td>\n",
       "      <td>0.897880</td>\n",
       "      <td>0.842770</td>\n",
       "      <td>0.856179</td>\n",
       "      <td>0.949265</td>\n",
       "      <td>0.950474</td>\n",
       "      <td>0.949020</td>\n",
       "      <td>0.975350</td>\n",
       "      <td>0.949719</td>\n",
       "      <td>0.893650</td>\n",
       "      <td>0.902314</td>\n",
       "      <td>0.758244</td>\n",
       "      <td>0.846654</td>\n",
       "      <td>0.887983</td>\n",
       "      <td>0.635515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.561000</td>\n",
       "      <td>1.296835</td>\n",
       "      <td>0.950948</td>\n",
       "      <td>0.893783</td>\n",
       "      <td>0.846942</td>\n",
       "      <td>0.860911</td>\n",
       "      <td>0.949855</td>\n",
       "      <td>0.950948</td>\n",
       "      <td>0.949735</td>\n",
       "      <td>0.977455</td>\n",
       "      <td>0.949719</td>\n",
       "      <td>0.877288</td>\n",
       "      <td>0.893195</td>\n",
       "      <td>0.801522</td>\n",
       "      <td>0.862729</td>\n",
       "      <td>0.886555</td>\n",
       "      <td>0.638828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.498600</td>\n",
       "      <td>1.266390</td>\n",
       "      <td>0.954028</td>\n",
       "      <td>0.913661</td>\n",
       "      <td>0.858461</td>\n",
       "      <td>0.875488</td>\n",
       "      <td>0.953555</td>\n",
       "      <td>0.954028</td>\n",
       "      <td>0.953008</td>\n",
       "      <td>0.979482</td>\n",
       "      <td>0.956741</td>\n",
       "      <td>0.904098</td>\n",
       "      <td>0.912758</td>\n",
       "      <td>0.798439</td>\n",
       "      <td>0.875257</td>\n",
       "      <td>0.881556</td>\n",
       "      <td>0.695573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>1.247788</td>\n",
       "      <td>0.955687</td>\n",
       "      <td>0.910708</td>\n",
       "      <td>0.874528</td>\n",
       "      <td>0.886149</td>\n",
       "      <td>0.955211</td>\n",
       "      <td>0.955687</td>\n",
       "      <td>0.954997</td>\n",
       "      <td>0.978419</td>\n",
       "      <td>0.952641</td>\n",
       "      <td>0.906155</td>\n",
       "      <td>0.903038</td>\n",
       "      <td>0.839612</td>\n",
       "      <td>0.870337</td>\n",
       "      <td>0.897282</td>\n",
       "      <td>0.741712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>1.236513</td>\n",
       "      <td>0.954621</td>\n",
       "      <td>0.910801</td>\n",
       "      <td>0.881029</td>\n",
       "      <td>0.892894</td>\n",
       "      <td>0.954464</td>\n",
       "      <td>0.954621</td>\n",
       "      <td>0.954207</td>\n",
       "      <td>0.975378</td>\n",
       "      <td>0.952641</td>\n",
       "      <td>0.893650</td>\n",
       "      <td>0.905183</td>\n",
       "      <td>0.898923</td>\n",
       "      <td>0.865713</td>\n",
       "      <td>0.897208</td>\n",
       "      <td>0.754459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>1.248393</td>\n",
       "      <td>0.955687</td>\n",
       "      <td>0.913928</td>\n",
       "      <td>0.877743</td>\n",
       "      <td>0.891187</td>\n",
       "      <td>0.955568</td>\n",
       "      <td>0.955687</td>\n",
       "      <td>0.955222</td>\n",
       "      <td>0.977352</td>\n",
       "      <td>0.957062</td>\n",
       "      <td>0.907152</td>\n",
       "      <td>0.906407</td>\n",
       "      <td>0.872431</td>\n",
       "      <td>0.866406</td>\n",
       "      <td>0.895592</td>\n",
       "      <td>0.747098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.203300</td>\n",
       "      <td>1.277964</td>\n",
       "      <td>0.956517</td>\n",
       "      <td>0.913204</td>\n",
       "      <td>0.891234</td>\n",
       "      <td>0.900388</td>\n",
       "      <td>0.956450</td>\n",
       "      <td>0.956517</td>\n",
       "      <td>0.956182</td>\n",
       "      <td>0.976363</td>\n",
       "      <td>0.950455</td>\n",
       "      <td>0.910012</td>\n",
       "      <td>0.911529</td>\n",
       "      <td>0.923866</td>\n",
       "      <td>0.871197</td>\n",
       "      <td>0.902112</td>\n",
       "      <td>0.757569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>1.261687</td>\n",
       "      <td>0.956754</td>\n",
       "      <td>0.911722</td>\n",
       "      <td>0.889182</td>\n",
       "      <td>0.898522</td>\n",
       "      <td>0.956728</td>\n",
       "      <td>0.956754</td>\n",
       "      <td>0.956467</td>\n",
       "      <td>0.975350</td>\n",
       "      <td>0.954844</td>\n",
       "      <td>0.893650</td>\n",
       "      <td>0.908711</td>\n",
       "      <td>0.925214</td>\n",
       "      <td>0.871287</td>\n",
       "      <td>0.906379</td>\n",
       "      <td>0.752741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.172800</td>\n",
       "      <td>1.250808</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.915218</td>\n",
       "      <td>0.891431</td>\n",
       "      <td>0.901257</td>\n",
       "      <td>0.957331</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.957033</td>\n",
       "      <td>0.975378</td>\n",
       "      <td>0.959296</td>\n",
       "      <td>0.910012</td>\n",
       "      <td>0.909961</td>\n",
       "      <td>0.924540</td>\n",
       "      <td>0.872323</td>\n",
       "      <td>0.905905</td>\n",
       "      <td>0.752640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.186700</td>\n",
       "      <td>1.260288</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.918145</td>\n",
       "      <td>0.886387</td>\n",
       "      <td>0.899222</td>\n",
       "      <td>0.957124</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.956905</td>\n",
       "      <td>0.976390</td>\n",
       "      <td>0.959296</td>\n",
       "      <td>0.916913</td>\n",
       "      <td>0.908711</td>\n",
       "      <td>0.900288</td>\n",
       "      <td>0.871872</td>\n",
       "      <td>0.904009</td>\n",
       "      <td>0.756296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 00:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model Performance on test dataset: \n",
      "{'eval_loss': 1.2566423416137695, 'eval_macro_acc': 0.9586894586894587, 'eval_macro_precision': 0.9098005480315166, 'eval_macro_recall': 0.8891243667621189, 'eval_macro_f1': 0.8976103070831851, 'eval_weighted_precision': 0.9584484113093603, 'eval_weighted_recall': 0.9586894586894587, 'eval_weighted_f1': 0.9583890595215988, 'eval_sdoh_community_present_macro_f1': 0.9824086949258307, 'eval_sdoh_community_absent_macro_f1': 0.941429498470381, 'eval_sdoh_education_macro_f1': 0.8968152866242038, 'eval_sdoh_economics_macro_f1': 0.9124918716761746, 'eval_sdoh_environment_macro_f1': 0.9162276768414875, 'eval_behavior_alcohol_macro_f1': 0.8556693389993264, 'eval_behavior_tobacco_macro_f1': 0.9245133722814941, 'eval_behavior_drug_macro_f1': 0.7513267168465827, 'eval_runtime': 54.9432, 'eval_samples_per_second': 19.165, 'eval_steps_per_second': 1.201, 'epoch': 15.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "test_metrics = trainer.evaluate(test_ds)\n",
    "print(\"Best model Performance on test dataset: \")\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aca7d8-27d5-459d-84e5-907edd75b79a",
   "metadata": {},
   "source": [
    "### 5.2 Bio-Clinical BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc51de6f-92b1-4aba-a7b6-00ac7ac0045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Bio-ClinicalBERT as model 2\n",
    "modelname2 = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f115d62-9e15-4d04-ad0e-fd586f642368",
   "metadata": {},
   "source": [
    "##### Convert these three datasets to Hugging Face Dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77e37ffe-a546-4887-97f8-b4e029697f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(data_train)\n",
    "val_ds = Dataset.from_pandas(data_val)\n",
    "test_ds = Dataset.from_pandas(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b04f24d-80b5-49fa-8133-1d34ae513b0a",
   "metadata": {},
   "source": [
    "### 5.2.1 Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c83aea47-fdcd-458c-8b84-a69fec637e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a1686a8ee34a18b70f79f1edecebbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4917 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28894dc9c26743a0ae73ce32d8d552bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1055 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2da5849479425ca5bea9f427d6e051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1053 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74aae16c71284f38a7e68ff660c62b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4917 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68dd4e3d42ed43eeb3549a50c0e98122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1055 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39b4d3c00a641e297435bfae70f9721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1053 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['social_history'], padding='max_length',truncation=True,max_length=128)\n",
    "    \n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "val_ds = val_ds.map(tokenize, batched=True)\n",
    "test_ds = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "def combine_labels(batch):\n",
    "    labels = []\n",
    "    for i in range(len(batch[full_cols[0]])):\n",
    "        row = []\n",
    "        for cols in full_cols:\n",
    "            row.append(batch[cols][i])\n",
    "        labels.append(row)\n",
    "    batch['labels'] = labels\n",
    "    return batch\n",
    "\n",
    "train_ds = train_ds.map(combine_labels, batched=True)\n",
    "val_ds = val_ds.map(combine_labels, batched=True)\n",
    "test_ds = test_ds.map(combine_labels, batched=True)\n",
    "\n",
    "\n",
    "# Remove extra columns to avoid passing extra columns to model\n",
    "use_cols=['input_ids', 'attention_mask', 'labels']\n",
    "train_ds = train_ds.remove_columns([cols for cols in train_ds.column_names if cols not in use_cols])\n",
    "val_ds = val_ds.remove_columns([cols for cols in val_ds.column_names if cols not in use_cols])\n",
    "test_ds = test_ds.remove_columns([cols for cols in test_ds.column_names if cols not in use_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425c6e87-c9e8-4975-8fed-b2eaad099fb0",
   "metadata": {},
   "source": [
    "### 5.2.2 Create the BioClinicalBERT Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7458200e-b640-4e5f-b94d-3f804750018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MultiTaskBertModel2 were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifiers.0.bias', 'classifiers.0.weight', 'classifiers.1.bias', 'classifiers.1.weight', 'classifiers.2.bias', 'classifiers.2.weight', 'classifiers.3.bias', 'classifiers.3.weight', 'classifiers.4.bias', 'classifiers.4.weight', 'classifiers.5.bias', 'classifiers.5.weight', 'classifiers.6.bias', 'classifiers.6.weight', 'classifiers.7.bias', 'classifiers.7.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class MultiTaskBertModel2(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_tasks = 8\n",
    "        # self.bert = BertModel(config) \n",
    "        self.bert = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', config=config)  # Use Bio+Clinical BERT from huggingface\n",
    "        \n",
    "        self.classifiers = nn.ModuleList([nn.Linear(config.hidden_size, num_labels) for num_labels in num_labels_list])\n",
    "        self.num_labels_list = num_labels_list\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "\n",
    "        logits_list = [classifier(pooled_output) for classifier in self.classifiers]\n",
    "\n",
    "        batch_size = pooled_output.size(0)\n",
    "        device = pooled_output.device\n",
    "\n",
    "        # Initialize logits with zeros, shape: (batch_size, num_tasks, max_labels)\n",
    "        logits = torch.zeros(batch_size, self.num_tasks, 5, device=device)\n",
    "        # logits = torch.full((batch_size, self.num_tasks,5), -1e9, device=device)\n",
    "        for i, logit_task in enumerate(logits_list):\n",
    "            num_labels = self.num_labels_list[i]\n",
    "            logits[:, i, :num_labels] = logit_task\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = 0\n",
    "        for i in range(self.num_tasks):\n",
    "            loss += loss_fct(logits[:, i, :self.num_labels_list[i]], labels[:, i])\n",
    "\n",
    "        return (loss, logits)\n",
    "\n",
    "config = AutoConfig.from_pretrained(modelname2)\n",
    "model = MultiTaskBertModel2.from_pretrained(modelname2, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17efeae3-241e-4976-b828-79ac8d072ad3",
   "metadata": {},
   "source": [
    "#### 5.2.3Set up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c7282c-41e5-4736-ad09-673235179134",
   "metadata": {},
   "outputs": [],
   "source": [
    "默认adam优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8ac7e73-ec2d-4d21-8872-e589900a15c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a few arguments in a seperate TrainingArguments class\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./Final_Project2',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='steps',    \n",
    "    logging_steps=50,\n",
    "    push_to_hub=False,\n",
    "    #report_to=\"tensorboard\",\n",
    "    #Evaluate after each epoch and save checkpoints to easily resume training, add epochs\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "\n",
    "    # Set to automatically load the the best macro_f1 score model on validation set\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='macro_f1',\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # tokenizer=tokenizer,\n",
    "    processing_class= tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce727971-bbf9-42f7-9443-c96c21ef7938",
   "metadata": {},
   "source": [
    "#### Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2634fe48-dde6-488b-b5f3-564e2cb41ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4620' max='4620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4620/4620 7:21:37, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro Acc</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Weighted Precision</th>\n",
       "      <th>Weighted Recall</th>\n",
       "      <th>Weighted F1</th>\n",
       "      <th>Sdoh Community Present Macro F1</th>\n",
       "      <th>Sdoh Community Absent Macro F1</th>\n",
       "      <th>Sdoh Education Macro F1</th>\n",
       "      <th>Sdoh Economics Macro F1</th>\n",
       "      <th>Sdoh Environment Macro F1</th>\n",
       "      <th>Behavior Alcohol Macro F1</th>\n",
       "      <th>Behavior Tobacco Macro F1</th>\n",
       "      <th>Behavior Drug Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.733800</td>\n",
       "      <td>3.480148</td>\n",
       "      <td>0.863863</td>\n",
       "      <td>0.583767</td>\n",
       "      <td>0.588099</td>\n",
       "      <td>0.576620</td>\n",
       "      <td>0.822185</td>\n",
       "      <td>0.863863</td>\n",
       "      <td>0.838668</td>\n",
       "      <td>0.885264</td>\n",
       "      <td>0.470647</td>\n",
       "      <td>0.492300</td>\n",
       "      <td>0.695792</td>\n",
       "      <td>0.613403</td>\n",
       "      <td>0.477653</td>\n",
       "      <td>0.603042</td>\n",
       "      <td>0.374857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.524200</td>\n",
       "      <td>2.347675</td>\n",
       "      <td>0.913744</td>\n",
       "      <td>0.830754</td>\n",
       "      <td>0.682979</td>\n",
       "      <td>0.706172</td>\n",
       "      <td>0.909429</td>\n",
       "      <td>0.913744</td>\n",
       "      <td>0.901270</td>\n",
       "      <td>0.968105</td>\n",
       "      <td>0.716197</td>\n",
       "      <td>0.651621</td>\n",
       "      <td>0.846188</td>\n",
       "      <td>0.642113</td>\n",
       "      <td>0.599974</td>\n",
       "      <td>0.757656</td>\n",
       "      <td>0.467522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.677500</td>\n",
       "      <td>1.763966</td>\n",
       "      <td>0.934953</td>\n",
       "      <td>0.853512</td>\n",
       "      <td>0.760982</td>\n",
       "      <td>0.787933</td>\n",
       "      <td>0.930510</td>\n",
       "      <td>0.934953</td>\n",
       "      <td>0.929175</td>\n",
       "      <td>0.970094</td>\n",
       "      <td>0.911619</td>\n",
       "      <td>0.777764</td>\n",
       "      <td>0.896979</td>\n",
       "      <td>0.643527</td>\n",
       "      <td>0.684037</td>\n",
       "      <td>0.846764</td>\n",
       "      <td>0.572679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.388700</td>\n",
       "      <td>1.480179</td>\n",
       "      <td>0.945142</td>\n",
       "      <td>0.860823</td>\n",
       "      <td>0.812089</td>\n",
       "      <td>0.830907</td>\n",
       "      <td>0.942533</td>\n",
       "      <td>0.945142</td>\n",
       "      <td>0.942727</td>\n",
       "      <td>0.981533</td>\n",
       "      <td>0.936312</td>\n",
       "      <td>0.841318</td>\n",
       "      <td>0.891117</td>\n",
       "      <td>0.643623</td>\n",
       "      <td>0.806188</td>\n",
       "      <td>0.868439</td>\n",
       "      <td>0.678723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.942500</td>\n",
       "      <td>1.373281</td>\n",
       "      <td>0.949526</td>\n",
       "      <td>0.882330</td>\n",
       "      <td>0.845580</td>\n",
       "      <td>0.858694</td>\n",
       "      <td>0.948452</td>\n",
       "      <td>0.949526</td>\n",
       "      <td>0.948355</td>\n",
       "      <td>0.982509</td>\n",
       "      <td>0.944517</td>\n",
       "      <td>0.883679</td>\n",
       "      <td>0.906433</td>\n",
       "      <td>0.742974</td>\n",
       "      <td>0.831188</td>\n",
       "      <td>0.869776</td>\n",
       "      <td>0.708479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.803500</td>\n",
       "      <td>1.277832</td>\n",
       "      <td>0.952725</td>\n",
       "      <td>0.912232</td>\n",
       "      <td>0.838079</td>\n",
       "      <td>0.855107</td>\n",
       "      <td>0.951953</td>\n",
       "      <td>0.952725</td>\n",
       "      <td>0.951129</td>\n",
       "      <td>0.977299</td>\n",
       "      <td>0.947128</td>\n",
       "      <td>0.846737</td>\n",
       "      <td>0.909935</td>\n",
       "      <td>0.709488</td>\n",
       "      <td>0.860093</td>\n",
       "      <td>0.882454</td>\n",
       "      <td>0.707727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.582100</td>\n",
       "      <td>1.227464</td>\n",
       "      <td>0.953910</td>\n",
       "      <td>0.907465</td>\n",
       "      <td>0.853427</td>\n",
       "      <td>0.867909</td>\n",
       "      <td>0.953077</td>\n",
       "      <td>0.953910</td>\n",
       "      <td>0.952794</td>\n",
       "      <td>0.982530</td>\n",
       "      <td>0.949342</td>\n",
       "      <td>0.893650</td>\n",
       "      <td>0.904754</td>\n",
       "      <td>0.759702</td>\n",
       "      <td>0.854005</td>\n",
       "      <td>0.879714</td>\n",
       "      <td>0.719575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.534100</td>\n",
       "      <td>1.235442</td>\n",
       "      <td>0.953791</td>\n",
       "      <td>0.916702</td>\n",
       "      <td>0.846153</td>\n",
       "      <td>0.862540</td>\n",
       "      <td>0.953465</td>\n",
       "      <td>0.953791</td>\n",
       "      <td>0.952585</td>\n",
       "      <td>0.977352</td>\n",
       "      <td>0.958680</td>\n",
       "      <td>0.889936</td>\n",
       "      <td>0.900669</td>\n",
       "      <td>0.706680</td>\n",
       "      <td>0.865426</td>\n",
       "      <td>0.887577</td>\n",
       "      <td>0.714001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.388600</td>\n",
       "      <td>1.225310</td>\n",
       "      <td>0.956398</td>\n",
       "      <td>0.924391</td>\n",
       "      <td>0.865781</td>\n",
       "      <td>0.885331</td>\n",
       "      <td>0.955921</td>\n",
       "      <td>0.956398</td>\n",
       "      <td>0.955541</td>\n",
       "      <td>0.976253</td>\n",
       "      <td>0.954166</td>\n",
       "      <td>0.911356</td>\n",
       "      <td>0.912837</td>\n",
       "      <td>0.802855</td>\n",
       "      <td>0.863520</td>\n",
       "      <td>0.896223</td>\n",
       "      <td>0.765433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.348700</td>\n",
       "      <td>1.206875</td>\n",
       "      <td>0.956872</td>\n",
       "      <td>0.919542</td>\n",
       "      <td>0.865574</td>\n",
       "      <td>0.883303</td>\n",
       "      <td>0.956306</td>\n",
       "      <td>0.956872</td>\n",
       "      <td>0.956113</td>\n",
       "      <td>0.979458</td>\n",
       "      <td>0.956741</td>\n",
       "      <td>0.893628</td>\n",
       "      <td>0.908801</td>\n",
       "      <td>0.802212</td>\n",
       "      <td>0.861524</td>\n",
       "      <td>0.899059</td>\n",
       "      <td>0.765003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.256100</td>\n",
       "      <td>1.263657</td>\n",
       "      <td>0.954621</td>\n",
       "      <td>0.905218</td>\n",
       "      <td>0.859731</td>\n",
       "      <td>0.870430</td>\n",
       "      <td>0.954302</td>\n",
       "      <td>0.954621</td>\n",
       "      <td>0.953786</td>\n",
       "      <td>0.976281</td>\n",
       "      <td>0.952291</td>\n",
       "      <td>0.880948</td>\n",
       "      <td>0.907523</td>\n",
       "      <td>0.760913</td>\n",
       "      <td>0.863376</td>\n",
       "      <td>0.886968</td>\n",
       "      <td>0.735142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>1.244344</td>\n",
       "      <td>0.955569</td>\n",
       "      <td>0.911384</td>\n",
       "      <td>0.867722</td>\n",
       "      <td>0.880998</td>\n",
       "      <td>0.955338</td>\n",
       "      <td>0.955569</td>\n",
       "      <td>0.954976</td>\n",
       "      <td>0.976309</td>\n",
       "      <td>0.951934</td>\n",
       "      <td>0.883679</td>\n",
       "      <td>0.908386</td>\n",
       "      <td>0.803423</td>\n",
       "      <td>0.852944</td>\n",
       "      <td>0.900458</td>\n",
       "      <td>0.770849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>1.240165</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.917461</td>\n",
       "      <td>0.870818</td>\n",
       "      <td>0.884989</td>\n",
       "      <td>0.957079</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.956669</td>\n",
       "      <td>0.979435</td>\n",
       "      <td>0.952291</td>\n",
       "      <td>0.890270</td>\n",
       "      <td>0.906525</td>\n",
       "      <td>0.804065</td>\n",
       "      <td>0.858092</td>\n",
       "      <td>0.908662</td>\n",
       "      <td>0.780570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.182800</td>\n",
       "      <td>1.251818</td>\n",
       "      <td>0.956280</td>\n",
       "      <td>0.914892</td>\n",
       "      <td>0.867247</td>\n",
       "      <td>0.882178</td>\n",
       "      <td>0.955797</td>\n",
       "      <td>0.956280</td>\n",
       "      <td>0.955532</td>\n",
       "      <td>0.977352</td>\n",
       "      <td>0.949719</td>\n",
       "      <td>0.890270</td>\n",
       "      <td>0.902388</td>\n",
       "      <td>0.803391</td>\n",
       "      <td>0.865761</td>\n",
       "      <td>0.902105</td>\n",
       "      <td>0.766436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.201800</td>\n",
       "      <td>1.254799</td>\n",
       "      <td>0.956398</td>\n",
       "      <td>0.916183</td>\n",
       "      <td>0.866970</td>\n",
       "      <td>0.882416</td>\n",
       "      <td>0.955935</td>\n",
       "      <td>0.956398</td>\n",
       "      <td>0.955617</td>\n",
       "      <td>0.977352</td>\n",
       "      <td>0.951934</td>\n",
       "      <td>0.890270</td>\n",
       "      <td>0.904379</td>\n",
       "      <td>0.804080</td>\n",
       "      <td>0.865761</td>\n",
       "      <td>0.900395</td>\n",
       "      <td>0.765159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 01:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test dataset: \n",
      "{'eval_loss': 1.252211570739746, 'eval_macro_acc': 0.9560778727445394, 'eval_macro_precision': 0.9073933692731901, 'eval_macro_recall': 0.8607660058002333, 'eval_macro_f1': 0.8727981196506881, 'eval_weighted_precision': 0.9554850126387042, 'eval_weighted_recall': 0.9560778727445394, 'eval_weighted_f1': 0.955216154636358, 'eval_sdoh_community_present_macro_f1': 0.9698462243349274, 'eval_sdoh_community_absent_macro_f1': 0.9267868730879762, 'eval_sdoh_education_macro_f1': 0.9033117042115573, 'eval_sdoh_economics_macro_f1': 0.9144219159207383, 'eval_sdoh_environment_macro_f1': 0.7741762648353204, 'eval_behavior_alcohol_macro_f1': 0.870937793832842, 'eval_behavior_tobacco_macro_f1': 0.9096333577625522, 'eval_behavior_drug_macro_f1': 0.713270823219591, 'eval_runtime': 73.8317, 'eval_samples_per_second': 14.262, 'eval_steps_per_second': 0.894, 'epoch': 15.0}\n"
     ]
    }
   ],
   "source": [
    "trainer2.train()\n",
    "\n",
    "test_metrics = trainer2.evaluate(test_ds)\n",
    "print(\"Performance on test dataset: \")\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8357bbe-07f5-4d44-a871-a5996fbdd98e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
